{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "B91ACBC33013437C8EE6EF3EDF645F96",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../input/\")\n",
    "import d2l_jay9460 as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float32\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8D94861525843EE86D09A3872717430",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## GRU\n",
    "\n",
    "$$\n",
    "R_{t} = σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r)\\\\    \n",
    "Z_{t} = σ(X_tW_{xz} + H_{t−1}W_{hz} + b_z)\\\\  \n",
    "\\widetilde{H}_t = tanh(X_tW_{xh} + (R_t ⊙H_{t−1})W_{hh} + b_h)\\\\\n",
    "H_t = Z_t⊙H_{t−1} + (1−Z_t)⊙\\widetilde{H}_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "08986FE27A23449F84C1EA7757FD3B08",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    def get_params(self):\n",
    "        def _set_W(shape):\n",
    "            W = torch.tensor(np.random.normal(0, 0.01, size=shape), device = device, dtype = dtype)\n",
    "            return nn.Parameter(W, requires_grad=True)\n",
    "            \n",
    "        def set_param():\n",
    "            W_x = _set_W(shape = (self.num_inputs, self.num_hiddens))\n",
    "            W_h = _set_W(shape = (self.num_hiddens, self.num_hiddens))\n",
    "            b   = nn.Parameter(torch.zeros(self.num_hiddens, device=device, dtype=dtype), requires_grad=True)\n",
    "            return (W_x, W_h, b)\n",
    "        \n",
    "        W_xr, W_hr, b_r = set_param()\n",
    "        W_xz, W_hz, b_z = set_param()\n",
    "        W_xh, W_hh, b_h = set_param()\n",
    "        \n",
    "        W_ho = _set_W(shape = (self.num_hiddens, self.num_outputs))\n",
    "        b_o  = nn.Parameter(torch.zeros(self.num_outputs, device=device, dtype=dtype), requires_grad=True)\n",
    "        return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_ho, b_o])\n",
    "    \n",
    "    def init_state(self, batch_size, num_hiddens, device=device):\n",
    "        return torch.zeros((batch_size, num_hiddens), device=device, dtype=dtype)\n",
    "        \n",
    "    def gru(self, inputs, state, params):\n",
    "        outputs = []\n",
    "        W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_ho, b_o = params\n",
    "        H = state\n",
    "        for X in inputs:\n",
    "            R = torch.sigmoid( torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r )\n",
    "            Z = torch.sigmoid( torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z )\n",
    "            H_candidate = torch.tanh( torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h )\n",
    "            H = Z * H + (1 - Z) * H_candidate\n",
    "            Y = torch.matmul(H, W_ho) + b_o\n",
    "            outputs.append(Y)\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "24837F8F6DF94A83903B016B57B9E70E",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1027]) torch.Size([1, 1027]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "num_hiddens = 256\n",
    "gruer = GRU(vocab_size, num_hiddens, vocab_size)\n",
    "params, state = gruer.get_params(), gruer.init_state(1, num_hiddens)\n",
    "# print(params,state.shape)\n",
    "\n",
    "gru = gruer.gru\n",
    "inputs = d2l.to_onehot(torch.tensor([[char_to_idx['分']]]), n_class = vocab_size)\n",
    "outputs, state = gru(inputs, state, params)\n",
    "\n",
    "print(inputs[0].shape, outputs[0].shape, state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "id": "AA44086B93E246D28E2E93D0BCE59B95",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 149.774903, time 1.18 sec\n",
      " - 分开 我不要 我不要你我想想你的可爱女人 坏坏的让我 爱爱人 我不要 我不要你我想想你的可爱女人 坏坏的\n",
      " - 不分开 我不要 我不要你我想想你的可爱女人 坏坏的让我 爱爱人 我不要 我不要你我想想你的可爱女人 坏坏的\n",
      "epoch 80, perplexity 32.686437, time 1.18 sec\n",
      " - 分开 我不能这样 你知不觉 你已经 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再\n",
      " - 不分开 爱你的美女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的\n",
      "epoch 120, perplexity 6.305357, time 1.20 sec\n",
      " - 分开我 不知不觉 我跟了这节奏 后知后觉 我该好好生活 我知不觉 你已经离开我 不知不觉 我跟了这节奏 \n",
      " - 不分开 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 我该好好生活 我知不觉 你已经离开我 不\n",
      "epoch 160, perplexity 2.040157, time 1.16 sec\n",
      " - 分开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 \n",
      " - 不分开 爱知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我\n"
     ]
    }
   ],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "d2l.train_and_predict_rnn(gru, gruer.get_params, gruer.init_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                          clipping_theta, batch_size, pred_period, pred_len,\n",
    "                          prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "id": "B9196755495C4CA1B49780CFF52B0110",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 1.024406, time 0.79 sec\n",
      " - 分开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让\n",
      " - 不分开球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别\n",
      "epoch 80, perplexity 1.011810, time 0.82 sec\n",
      " - 分开始共渡每一天 手牵手 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看\n",
      " - 不分开 没有你以  我想大声宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵\n",
      "epoch 120, perplexity 1.015511, time 0.89 sec\n",
      " - 分开球我爱你看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害\n",
      " - 不分开 陷入了危险边缘Baby  我的世界已狂风暴雨 Wu  爱情来的太快就像龙卷风 离不开暴风圈来不及逃\n",
      "epoch 160, perplexity 1.010384, time 0.96 sec\n",
      " - 分开 我爱你看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害\n",
      " - 不分开 你爱我 开不了口 周杰伦 才离开没多久就开始 担心今天的你过得好不好 整个画面是你 想你想的睡不著\n"
     ]
    }
   ],
   "source": [
    "#简洁实现\n",
    "\n",
    "num_hiddens=256\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "lr = 1e-2 # 注意调整学习率\n",
    "gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, vocab_size).to(device)\n",
    "d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "893B62BC691B42CA8CD79D6D968C7735",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "\n",
    "$$\n",
    "I_t = σ(X_tW_{xi} + H_{t−1}W_{hi} + b_i) \\\\\n",
    "F_t = σ(X_tW_{xf} + H_{t−1}W_{hf} + b_f)\\\\\n",
    "O_t = σ(X_tW_{xo} + H_{t−1}W_{ho} + b_o)\\\\\n",
    "\\widetilde{C}_t = tanh(X_tW_{xc} + H_{t−1}W_{hc} + b_c)\\\\\n",
    "C_t = F_t ⊙C_{t−1} + I_t ⊙\\widetilde{C}_t\\\\\n",
    "H_t = O_t⊙tanh(C_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "id": "0DE783939B284177BD0C1677DDFA950A",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "    \n",
    "    def get_params(self):\n",
    "        def _set_W(shape):\n",
    "            W = torch.tensor(np.random.normal(0, 0.01, size=shape), device = device, dtype = dtype)\n",
    "            return nn.Parameter(W, requires_grad=True)\n",
    "            \n",
    "        def set_param():\n",
    "            W_x = _set_W(shape = (self.num_inputs, self.num_hiddens))\n",
    "            W_h = _set_W(shape = (self.num_hiddens, self.num_hiddens))\n",
    "            b   = nn.Parameter(torch.zeros(self.num_hiddens, device=device, dtype=dtype), requires_grad=True)\n",
    "            return (W_x, W_h, b)\n",
    "        \n",
    "        W_xi, W_hi, b_i = set_param()\n",
    "        W_xf, W_hf, b_f = set_param()\n",
    "        W_xo, W_ho, b_o = set_param()\n",
    "        W_xc, W_hc, b_c = set_param()\n",
    "        \n",
    "        W_hq = _set_W(shape = (self.num_hiddens, self.num_outputs) )\n",
    "        b_q  = nn.Parameter(torch.zeros(self.num_outputs, device=device, dtype=dtype), requires_grad=True)\n",
    "        return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])\n",
    "    \n",
    "    def lstm(self, inputs, state, params):\n",
    "        W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params\n",
    "        H, C = state\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid( torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i )\n",
    "            F = torch.sigmoid( torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f )\n",
    "            O = torch.sigmoid( torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o )\n",
    "            C_candidate = torch.tanh( torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c )\n",
    "            C = F * C + I * C_candidate\n",
    "            H = O * torch.tanh(C)\n",
    "            Y = torch.matmul(H, W_hq) + b_q\n",
    "            outputs.append(Y)\n",
    "        return outputs, (H, C)\n",
    "    \n",
    "    def init_state(self, batch_size, num_outputs, device = device):\n",
    "        H = torch.zeros((batch_size, num_outputs), dtype=dtype, device=device)\n",
    "        C = torch.zeros((batch_size, num_outputs), dtype=dtype, device=device)\n",
    "        return (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "id": "4AE74793334140A0AA97EBD815F6693F",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1027]) torch.Size([1, 1027]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "num_hiddens = 256\n",
    "lstmer = LSTM(vocab_size, num_hiddens, vocab_size)\n",
    "params, state = lstmer.get_params(), lstmer.init_state(1, num_hiddens)\n",
    "# print(params,state[0].shape)\n",
    "\n",
    "lstm = lstmer.lstm\n",
    "inputs = d2l.to_onehot(torch.tensor([[char_to_idx['分']]]), n_class = vocab_size)\n",
    "outputs, state = lstm(inputs, state, params)\n",
    "\n",
    "print(inputs[0].shape, outputs[0].shape, state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "id": "98BEDDA43FB74800960213A32A99FCD2",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 204.493270, time 1.28 sec\n",
      " - 分开 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我\n",
      " - 不分开 我不的我 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 我不的 \n",
      "epoch 80, perplexity 75.061221, time 1.29 sec\n",
      " - 分开 我想 你你的爱你 你知  你不 我不要 我不要 我不要 你不 我不要 你不 我不要 我不要 你不 \n",
      " - 不分开 我想 你你的爱你 你知  你不 我不要 我不要 我不要 你不 我不要 你不 我不要 我不要 你不 \n",
      "epoch 120, perplexity 22.797335, time 1.28 sec\n",
      " - 分开 我爱你的爱笑 就知的美 快你的美 在小村  什么我 有有我 有你么 我不要 我不要 我不好 是你的\n",
      " - 不分开 我不了这样 我知了这样 我知了这生 我的让我 你你的让我面红的可爱女人 坏柔的让我心疼的可爱女人 \n",
      "epoch 160, perplexity 7.384752, time 1.29 sec\n",
      " - 分开 我不要再想 我不 我不 我不要 爱情走的太快就像龙卷风 不能承受我已无处可可躲 我不要再想 我不能\n",
      " - 不分开 我知不能 你情的让我爱红 可爱女人 经已已可可简 后以透透我 是你的美我 说伤寄美 如果什么我出的\n"
     ]
    }
   ],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "d2l.train_and_predict_rnn(lstm, lstmer.get_params, lstmer.init_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                          clipping_theta, batch_size, pred_period, pred_len,\n",
    "                          prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "id": "F300B84C31164B22837F5F973E14DC8F",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 1.022268, time 1.06 sec\n",
      " - 分开 我不能看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害\n",
      " - 不分开 我不能回到你 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处\n",
      "epoch 80, perplexity 1.029295, time 1.11 sec\n",
      " - 分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专\n",
      " - 不分开 我不能回到你身边 我给你的爱写在西元前 深埋在美索不达米亚平原 几十个世纪后出土发现 泥板上的字迹\n",
      "epoch 120, perplexity 1.011268, time 1.02 sec\n",
      " - 分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专\n",
      " - 不分开 我不能回到 我不 我不 我不要再想你 爱情来的太快就像龙卷风 离不开暴风圈来不及逃 我不能再想 我\n",
      "epoch 160, perplexity 1.008309, time 1.03 sec\n",
      " - 分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专\n",
      " - 不分开 爱能不能够永远单纯没有悲哀 我 想带你骑单车 我 想和你看棒球 想这样没担忧 唱着歌 一直走 我想\n"
     ]
    }
   ],
   "source": [
    "#简洁实现\n",
    "\n",
    "num_hiddens=256\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "lr = 1e-2 # 注意调整学习率\n",
    "lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = d2l.RNNModel(lstm_layer, vocab_size)\n",
    "d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D51FB52CE9E24726AA710C949D63634A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Deep RNN\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)} + \\boldsymbol{b}_h^{(1)})\\\\\n",
    "\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)} + \\boldsymbol{b}_h^{(\\ell)})\\\\\n",
    "\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "id": "CB4D2543D3024B7B851551D75D476856",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 1.351494, time 1.46 sec\n",
      " - 分开 这故事 告诉我 别怪我 说你怎么面对我 别怪我 说你怎么面对我 别怪我 说你怎么面对我 别怪我 说\n",
      " - 不分开却已了伊斯坦堡 就只想你开 我不  那处悲剧 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害\n",
      "epoch 80, perplexity 1.014319, time 1.49 sec\n",
      " - 分开都妈出看 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯\n",
      " - 不分开不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 其实我回家就想要阻止一切\n",
      "epoch 120, perplexity 1.013718, time 1.47 sec\n",
      " - 分开的直走 我想很伊斯坦堡 就像是童话故事  有教堂有城堡 每天忙碌地的寻找 到底什么我想要 却发现迷了\n",
      " - 不分开不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 我叫你爸 你打我妈 这样\n",
      "epoch 160, perplexity 1.012212, time 1.47 sec\n",
      " - 分开已经发出痛 有什么不妥 有话就直说 别窝在角落 不爽就反驳 到底拽什么 懂不懂篮球 有种不要走 三对\n",
      " - 不分开不要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 我叫你爸 你打我妈 这样\n"
     ]
    }
   ],
   "source": [
    "num_hiddens=256\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "lr = 1e-2 # 注意调整学习率\n",
    "\n",
    "gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=2)\n",
    "model = d2l.RNNModel(gru_layer, vocab_size).to(device)\n",
    "d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCFCD35E996C4ABA8FE1CE4684EA9EBA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)})\\\\\n",
    "\\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}) \\end{aligned} $$\n",
    "$$\n",
    "\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}_{t}, \\overleftarrow{\\boldsymbol{H}}_t)\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "E3B22433564848939A70A67204E9C8E1",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_hiddens=128\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e-2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "lr = 1e-2 # 注意调整学习率\n",
    "\n",
    "gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=True)\n",
    "model = d2l.RNNModel(gru_layer, vocab_size).to(device)\n",
    "d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46A281FB0AF14D4FB02F2B42D7CFF46B",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
