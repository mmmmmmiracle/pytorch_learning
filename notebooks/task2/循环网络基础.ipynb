{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B62A1FAEE064121821A051D3A6E7BA9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 循环神经网络的构造\n",
    "\n",
    "我们先看循环神经网络的具体构造。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是时间步$t$的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量，则：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).\n",
    "$$\n",
    "\n",
    "\n",
    "其中，$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，$\\boldsymbol{b}_{h} \\in \\mathbb{R}^{1 \\times h}$，$\\phi$函数是非线性激活函数。由于引入了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$，$H_{t}$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。\n",
    "\n",
    "在时间步$t$，输出层的输出为：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$，$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "AE8BD0560C1F4A728630F6C8C8C0DB29",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2l_jay9460 as d2l\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float32\n",
    "vocab_size = 1027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9903BA4367A8464A9253640830353657",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 手写实现循环网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "B8F68C0A7080465485E438E4573FF8C4",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot\n",
    "def onehot(x, num_class):\n",
    "    res = torch.zeros(x.shape[0], num_class, device = device, dtype = dtype)\n",
    "    res.scatter_(dim=1, index = x.long().view(-1, 1), value=1)\n",
    "    return res\n",
    "    \n",
    "def to_onehot(X, num_class):\n",
    "    return [ onehot(X[:, i], num_class) for i in range(X.shape[1]) ]\n",
    "\n",
    "# onehot(torch.tensor(list(range(10)), dtype=dtype), 10)\n",
    "to_onehot(torch.arange(10).view(2, 5), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83AA64B01AE7417DB3C07A7ABC8C1124",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### RNN类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "3E4DD9017F244C77913F7CA80081E5EA",
    "jupyter": {
     "outputs_hidden": false
    },
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        self.params = self.get_params(num_inputs, num_hiddens, num_outputs)\n",
    "    \n",
    "    # 初始化隐藏状态\n",
    "    def init_state(self,batch_size, num_hiddens):\n",
    "        return torch.zeros((batch_size, num_hiddens), device=device)\n",
    "    \n",
    "    # 初始化模型参数\n",
    "    def get_params(self, num_inputs, num_hiddens, num_outputs):\n",
    "        def set_param(shape):\n",
    "            param = torch.zeros(shape, dtype = dtype, device = device)\n",
    "            nn.init.normal_(param, mean = 0, std = 0.01)\n",
    "            return nn.Parameter(param)\n",
    "        \n",
    "        W_xh = set_param((num_inputs, num_hiddens))\n",
    "        W_hh = set_param((num_hiddens, num_hiddens))\n",
    "        b_h  = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype = dtype))\n",
    "        W_ho = set_param((num_hiddens, num_outputs))\n",
    "        b_o  = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype = dtype))\n",
    "        return W_xh, W_hh, b_h, W_ho, b_o\n",
    "    \n",
    "    # 定义模型\n",
    "    def rnn(self, inputs, state, params):\n",
    "        W_xh, W_hh, b_h, W_ho, b_o = params\n",
    "        H = state\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            H = torch.tanh(torch.matmul(X,W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "            Y = torch.matmul(H, W_ho) + b_o\n",
    "            outputs.append(Y)\n",
    "        return outputs, (H, )\n",
    "        \n",
    "    # 梯度裁剪\n",
    "    def grad_clipping(self, params, theta):\n",
    "        norm = torch.tensor([0.0], device=device)\n",
    "        for param in params:\n",
    "            norm += (param.grad.data ** 2).sum()\n",
    "        norm = norm.sqrt().item()\n",
    "        if norm > theta:\n",
    "            for param in params:\n",
    "                param.grad.data *= (theta / norm)\n",
    "\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "# rnner = RNN(num_inputs, num_hiddens, num_outputs)\n",
    "# params, rnn = rnner.params, rnner.rnn\n",
    "# X = torch.arange(10).view(2, 5).to(device)\n",
    "# state = rnner.init_state(X.shape[0], num_hiddens)\n",
    "# inputs = to_onehot(X, vocab_size)\n",
    "# output, (state, ) = rnn(inputs, state, params)\n",
    "# print('num steps:',len(output), ' hidden states shape:', state.shape)\n",
    "# print(output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B732EC48C74D4D9C978C3E9F401E1604",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 定义预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "B1BB91CEBA434A14A5C3761065D86E24",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_state):\n",
    "    state = init_state(1, num_hiddens)\n",
    "    # 利用prefix的信息不断更新state\n",
    "    Y = None\n",
    "    for word in prefix:\n",
    "        X = to_onehot(torch.tensor([[char_to_idx[word]]]), num_class=vocab_size)\n",
    "        Y, (state, ) = rnn(X, state, params)\n",
    "    #预测\n",
    "    prefix += idx_to_char[Y[0].argmax(dim=1).item()]\n",
    "    # print(prefix)\n",
    "    while num_chars > 0:\n",
    "        X = to_onehot(torch.tensor([[char_to_idx[prefix[-1]]]]), num_class=vocab_size)\n",
    "        Y, (state, ) = rnn(X, state, params)\n",
    "        prefix += idx_to_char[Y[0].argmax(dim=1).item()]\n",
    "        num_chars -= 1\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "id": "8B97E3B88DCC415A896C75F6953B5742",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分开等他危画问距忙阳站外死\n",
      "不分开腮是叫几子邂感坟哀寞司\n",
      "旁边蛇实待密沙坏干蟑鸠补W\n"
     ]
    }
   ],
   "source": [
    "rnner = RNN(num_inputs, num_hiddens, num_outputs)\n",
    "params, rnn = rnner.params, rnner.rnn\n",
    "print(predict_rnn('分开', 10, rnn, params, rnner.init_state))\n",
    "\n",
    "rnner = RNN(num_inputs, num_hiddens, num_outputs)\n",
    "params, rnn = rnner.params, rnner.rnn\n",
    "print(predict_rnn('不分开', 10, rnn, params, rnner.init_state))\n",
    "\n",
    "rnner = RNN(num_inputs, num_hiddens, num_outputs)\n",
    "params, rnn = rnner.params, rnner.rnn\n",
    "print(predict_rnn('旁边', 10, rnn, params, rnner.init_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "id": "12243C4C16F248808A08FBCF70AD7B0E",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "rnner = RNN(num_inputs, num_hiddens, num_outputs)\n",
    "params, rnn = rnner.params, rnner.rnn\n",
    "def train_and_predict_rnn(rnner, is_random_iter):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:\n",
    "            state = rnner.init_state(batch_size, num_hiddens)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n",
    "        \n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = rnner.init_state(batch_size, num_hiddens)\n",
    "            else:  # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            inputs = to_onehot(X, vocab_size)\n",
    "            outputs, (state, ) = rnn(inputs, state, params)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            y = torch.flatten(Y.T)\n",
    "            l = loss(outputs, y.long())\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward(retain_graph=True)\n",
    "            rnner.grad_clipping(params, clipping_theta)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, rnner.init_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "id": "507C6121FC674B6182D6F2F5F2F00D43",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 71.522363, time 0.79 sec\n",
      " - 分开 我想要这生 我不要这 你有我 别你我 我不要你的 我不能 你爱我不 你想你 你爱我 我不要你的 我不\n",
      " - 不分开 我想要这生 我不要这 你有我 别你我 我不要你的 我不能 你爱我不 你想你 你爱我 我不要你的 我不\n",
      "epoch 100, perplexity 10.305623, time 0.75 sec\n",
      " - 分开 有使我 娘不依 快步默 一步四颗 连成线 的路段 三间四人 在小之 快沉默 娘子四颗 在头主 的路段\n",
      " - 不分开永 我有你烦 你知我 别怪我 娘子却人 在指忆 的片段 三一些风慢 老唱苦 旧皮堂 装属都有 连头的 \n",
      "epoch 150, perplexity 2.958872, time 0.72 sec\n",
      " - 分开 有什么 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星是否听\n",
      " - 不分开吗 我已你爸 你知我妈 这样了看怎么慢慢 就想开不之口让她知道 我一定会呵护著你 手著你说 你对我有多\n",
      "epoch 200, perplexity 1.605667, time 0.76 sec\n",
      " - 分开 有什去 一步两步三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背 默荡在蓝安排 连攻抢邻池里 \n",
      " - 不分开期 我叫你爸 你打我妈 这样对吗去嘛这样 还必让不牵鼻子走 瞎 太狼险的 快时光 分怎堂囱术的老板 练\n",
      "epoch 250, perplexity 1.314792, time 0.78 sec\n",
      " - 分开球了像想的我 在小村 我给还 穿发却依旧每日折一枝杨柳 在小村外的溪边河口 默默地你 泪去会枪 硬底子\n",
      " - 不分开期 我叫你爸 你打我妈 这样对吗干嘛这样 何必让酒牵鼻子走 瞎 说午险的 不的事我担朋 静属于头的我 \n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnner, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "id": "95CC39353E7B4C798CAFA265464BB334",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 2721.856866, time 2.80 sec\n",
      " - 分开  说                                                \n",
      " - 不分开                                                   \n",
      "epoch 100, perplexity 72.404022, time 1.22 sec\n",
      " - 分开的黑实一人的可爱女人啦过我不轻不可一步的在斑鸠的风不下不著不能不的我面女的可爱女人我想能的可我面狂的可\n",
      " - 不分开吗个打我想要你的微我不狂的可爱女人我的你的手怎著的风娘下的可爱女人我想想的可爱女人龙坏坏不多我想你的爱\n",
      "epoch 150, perplexity 95.646535, time 1.21 sec\n",
      " - 分开 出只还会想你让我有见活一多的手快每了有过你在它都着日有多你手在都我有无头着多的手快在人的手斑女人的手\n",
      " - 不分开觉 我不多的可样依默的我怎么人的口子在默我想你开 不过我有多再着多都想一我的可爱女人我想明的可样在默默\n",
      "epoch 200, perplexity 163.254149, time 1.18 sec\n",
      " - 分开 她色 会天不起 你跟 你著起你 你堡 你子休你 你知 你子休你 你知不起 你跟了 想你 不起你 你你\n",
      " - 不分开觉 你想 快一起你一你  你子了一天 你跟已你 你 心你 你一休 不爱了一起跟 已什的让你心 不想 一\n",
      "epoch 250, perplexity 276.336192, time 1.21 sec\n",
      " - 分开  不知我 我 一 我有多  我 我 我 一沉的有  一样着 我 一 我有一的我  我不要 我  这样\n",
      " - 不分开觉 却多透和不  一是一 一 我有多 一 我 一实我 一 是不  一是我 一 我有一  我 我 我 一\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnner, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A6D18FD5C0944D08882AA103DD2016A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 使用pytorch简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E51EFD551174BE38D917369E3CD11A4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "id": "CF61A6B4F16B4904BB127253CDD5942C",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "num_steps, batch_size = 35, 2\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Linear(self.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # inputs.shape: (batch_size, num_steps)\n",
    "        X = to_onehot(inputs, vocab_size)\n",
    "        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)\n",
    "        hiddens, state = self.rnn(X, state)\n",
    "        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)\n",
    "        output = self.dense(hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFC6FD34790746A9991F079A22313263",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "id": "D521E54A4C1443E981D3BC3DE19E1054",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开听斜斜忆门代斜斜忆门'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n",
    "                      char_to_idx):\n",
    "    state = None\n",
    "    output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = torch.tensor([output[-1]], device=device).view(1, 1)\n",
    "        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(Y.argmax(dim=1).item())\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "    \n",
    "model = RNNModel(rnn_layer, vocab_size).to(device)\n",
    "predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "id": "3BC3DDA992EA44EB990C458DBCEA327C",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size, pred_period, pred_len, prefixes):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\n",
    "        state = None\n",
    "        for X, Y in data_iter:\n",
    "            if state is not None:\n",
    "                # 使用detach函数从计算图分离隐藏状态\n",
    "                if isinstance (state, tuple): # LSTM, state:(h, c)  \n",
    "                    state[0].detach_()\n",
    "                    state[1].detach_()\n",
    "                else: \n",
    "                    state.detach_()\n",
    "            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)\n",
    "            y = torch.flatten(Y.T)\n",
    "            l = loss(output, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            rnner.grad_clipping(model.parameters(), clipping_theta)\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn_pytorch(\n",
    "                    prefix, pred_len, model, vocab_size, device, idx_to_char,\n",
    "                    char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "id": "3A2CF98170E244CB87B9D918E89D04B2",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 11.038127, time 0.52 sec\n",
      " - 分开 我有你的爱我 一个着 你说的美主 一枝杨柳 你在那里 你想要再不要 不想我不多你不 我不要再想你 \n",
      " - 不分开 我想要你不多 我 就你了你我不能 想要你的让我面红的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我\n",
      "epoch 100, perplexity 1.257996, time 0.67 sec\n",
      " - 分开 我来了太快就我想要你的微笑每天都能的爱你 我知道你 我面听的 何像了 什么我有多难熬多 没有你有不\n",
      " - 不分开 不能承受我已无处可躲 我不要再想 我不要再想 我不 我不 我不要再想你 不要再想 我不 我不 我不\n",
      "epoch 150, perplexity 1.065657, time 0.59 sec\n",
      " - 分开 我来了太快地我感动 河边的我 还是一定会 它一定中篮板 有它一直落口  是开 心话 这样的节奏 我\n",
      " - 不分开 不能 这样打我妈手 我说啊 你不那有我想要你不你 我 我你想你 不知不觉 你已经离开我 不知不觉 \n",
      "epoch 200, perplexity 1.032287, time 0.59 sec\n",
      " - 分开 我不了太多 我想一直说你听  着头 这故事 告诉我 印地安的传说 还真是 瞎透了 什么都有 这故事\n",
      " - 不分开 不能承受快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 习武之人切记 仁者无敌 是谁在练太极 风生\n",
      "epoch 250, perplexity 1.020809, time 0.62 sec\n",
      " - 分开 我不了太多地我想一直到以听 它一定一切得载著  这感动 三颗四颗 连成线一步两步三步四步望著天 看\n",
      " - 不分开 不能承受快已无处可躲 我不要再想 我不要再想 我不 我不 我不要再想你 不知不觉 你已经离开我 不\n"
     ]
    }
   ],
   "source": [
    "num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\n",
    "                            num_epochs, num_steps, lr, clipping_theta,\n",
    "                            batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1D37A181C3C481B8E4E64B2774278CB",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
