{"cells":[{"metadata":{"cell_type":"code","id":"41E5AE1F3B0E4C47816ED80D16D29DC5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 1.语言模型\n假设序列$w_1, w_2, \\ldots, w_T$中的每个词是依次生成的，我们有\n\n\n$$\n\n\\begin{align*}\nP(w_1, w_2, \\ldots, w_T)\n&= \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1})\\\\\n&= P(w_1)P(w_2 \\mid w_1) \\cdots P(w_T \\mid w_1w_2\\cdots w_{T-1})\n\\end{align*}\n\n$$\n\n\n例如，一段含有4个词的文本序列的概率\n\n\n$$\nP(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3).\n$$\n\n\n语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：\n\n\n$$\n\n\\hat P(w_1) = \\frac{n(w_1)}{n}\n\n$$\n\n\n其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。\n\n类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：\n\n\n$$\n\n\\hat P(w_2 \\mid w_1) = \\frac{n(w_1, w_2)}{n(w_1)}\n\n$$\n\n\n其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。\n\n## 2.n-gram\n序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \\mid w_1, w_2) = P(w_3 \\mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为\n\n\n$$\nP(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .\n$$\n\n\n以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：\n\n\n$$\n\n\\begin{align*}\nP(w_1, w_2, w_3, w_4)\n&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3)\\\\\n&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3)\n\\end{align*}\n\n$$\n\n\n当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为\n\n\n$$\n\n\\begin{aligned}\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\n\\end{aligned}\n\n$$\n\n\n当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。"},{"metadata":{"id":"ABCFCD19961C49F384C81C40F822F8C2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 3.数据集"},{"metadata":{"id":"B538EFBFF9024379B4A916963C4E06B4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def load_data_jay_lyrics():\n    with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n        corpus_chars = f.read()\n    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n    corpus_chars = corpus_chars[0:10000]\n    idx_to_char = list(set(corpus_chars))\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n    vocab_size = len(char_to_idx)\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size\ncorpus_indices, char_to_idx, idx_to_char, vocab_size = load_data_jay_lyrics()","execution_count":6},{"metadata":{"id":"FD6B36C53A7F48DE9B036D4AA5D33086","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 4.时序数据采样\n### 4.1 随机采样\n相邻的两个随机小批量在原始序列上的位置不一定相毗邻。"},{"metadata":{"id":"3BD3A04AFD7047918215B1D09ED60905","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X: tensor([[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]]), \n Y: tensor([[13, 14, 15, 16, 17, 18],\n        [19, 20, 21, 22, 23, 24]])\nX: tensor([[ 6,  7,  8,  9, 10, 11],\n        [ 0,  1,  2,  3,  4,  5]]), \n Y: tensor([[ 7,  8,  9, 10, 11, 12],\n        [ 1,  2,  3,  4,  5,  6]])\n","name":"stdout"}],"source":"import torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标\n    random.shuffle(example_indices)\n\n    def _data(i):\n        # 返回从i开始的长为num_steps的序列\n        return corpus_indices[i: i + num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个随机样本\n        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标\n        X = [_data(j) for j in batch_indices]\n        Y = [_data(j + 1) for j in batch_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n        \nfor X, Y in data_iter_random([i for i in range(30)], 2, 6):\n    print(f'X: {X}, \\n Y: {Y}')","execution_count":9},{"metadata":{"id":"0C9CFAD0E4FD4B2FB2D70125C4B4706F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 4.2相邻采样\n在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。"},{"metadata":{"id":"EA67BA25032B44179774197F67761D0C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X: tensor([[ 0,  1,  2,  3,  4,  5],\n        [15, 16, 17, 18, 19, 20]]), \n Y: tensor([[ 1,  2,  3,  4,  5,  6],\n        [16, 17, 18, 19, 20, 21]])\nX: tensor([[ 6,  7,  8,  9, 10, 11],\n        [21, 22, 23, 24, 25, 26]]), \n Y: tensor([[ 7,  8,  9, 10, 11, 12],\n        [22, 23, 24, 25, 26, 27]])\n","name":"stdout"}],"source":"def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n    indices = torch.tensor(corpus_indices, device=device)\n    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        X = indices[:, i: i + num_steps]\n        Y = indices[:, i + 1: i + num_steps + 1]\n        yield X, Y\n        \nfor X, Y in data_iter_consecutive([i for i in range(30)], 2, 6):\n    print(f'X: {X}, \\n Y: {Y}')","execution_count":10},{"metadata":{"id":"00E11DFEBE6048A3BD2F1FF9B95D405A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}